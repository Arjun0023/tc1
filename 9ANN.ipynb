{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputNeurons=2 \n",
    "hiddenlayerNeurons=4 \n",
    "outputNeurons=2 \n",
    "iteration=6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.randint(1,5,inputNeurons) \n",
    "output = np.array([1.0,0.0]) \n",
    "hidden_layer=np.random.rand(1,hiddenlayerNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_biass=np.random.rand(1,hiddenlayerNeurons) \n",
    "output_bias=np.random.rand(1,outputNeurons) \n",
    "hidden_weights=np.random.rand(inputNeurons,hiddenlayerNeurons) \n",
    "output_weights=np.random.rand(hiddenlayerNeurons,outputNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (layer):\n",
    "    return 1/(1 + np.exp(-layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(layer): \n",
    "    return layer*(1-layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "iteration: 0 :::: [[ 0.13681329 -0.77322914]]\n",
      "###output######## [[0.86318671 0.77322914]]\n",
      "**********************\n",
      "iteration: 1 :::: [[ 0.13646499 -0.76918338]]\n",
      "###output######## [[0.86353501 0.76918338]]\n",
      "**********************\n",
      "iteration: 2 :::: [[ 0.13611716 -0.76505684]]\n",
      "###output######## [[0.86388284 0.76505684]]\n",
      "**********************\n",
      "iteration: 3 :::: [[ 0.13576974 -0.76084921]]\n",
      "###output######## [[0.86423026 0.76084921]]\n",
      "**********************\n",
      "iteration: 4 :::: [[ 0.13542271 -0.75656034]]\n",
      "###output######## [[0.86457729 0.75656034]]\n",
      "**********************\n",
      "iteration: 5 :::: [[ 0.13507602 -0.75219018]]\n",
      "###output######## [[0.86492398 0.75219018]]\n",
      "**********************\n",
      "iteration: 6 :::: [[ 0.13472963 -0.74773884]]\n",
      "###output######## [[0.86527037 0.74773884]]\n",
      "**********************\n",
      "iteration: 7 :::: [[ 0.13438351 -0.74320659]]\n",
      "###output######## [[0.86561649 0.74320659]]\n",
      "**********************\n",
      "iteration: 8 :::: [[ 0.13403763 -0.73859383]]\n",
      "###output######## [[0.86596237 0.73859383]]\n",
      "**********************\n",
      "iteration: 9 :::: [[ 0.13369196 -0.73390116]]\n",
      "###output######## [[0.86630804 0.73390116]]\n",
      "**********************\n",
      "iteration: 10 :::: [[ 0.13334647 -0.72912934]]\n",
      "###output######## [[0.86665353 0.72912934]]\n",
      "**********************\n",
      "iteration: 11 :::: [[ 0.13300114 -0.72427932]]\n",
      "###output######## [[0.86699886 0.72427932]]\n",
      "**********************\n",
      "iteration: 12 :::: [[ 0.13265596 -0.71935222]]\n",
      "###output######## [[0.86734404 0.71935222]]\n",
      "**********************\n",
      "iteration: 13 :::: [[ 0.13231092 -0.71434936]]\n",
      "###output######## [[0.86768908 0.71434936]]\n",
      "**********************\n",
      "iteration: 14 :::: [[ 0.13196599 -0.70927226]]\n",
      "###output######## [[0.86803401 0.70927226]]\n",
      "**********************\n",
      "iteration: 15 :::: [[ 0.13162118 -0.70412264]]\n",
      "###output######## [[0.86837882 0.70412264]]\n",
      "**********************\n",
      "iteration: 16 :::: [[ 0.13127648 -0.69890241]]\n",
      "###output######## [[0.86872352 0.69890241]]\n",
      "**********************\n",
      "iteration: 17 :::: [[ 0.1309319  -0.69361369]]\n",
      "###output######## [[0.8690681  0.69361369]]\n",
      "**********************\n",
      "iteration: 18 :::: [[ 0.13058744 -0.6882588 ]]\n",
      "###output######## [[0.86941256 0.6882588 ]]\n",
      "**********************\n",
      "iteration: 19 :::: [[ 0.13024312 -0.68284026]]\n",
      "###output######## [[0.86975688 0.68284026]]\n",
      "**********************\n",
      "iteration: 20 :::: [[ 0.12989894 -0.67736077]]\n",
      "###output######## [[0.87010106 0.67736077]]\n",
      "**********************\n",
      "iteration: 21 :::: [[ 0.12955493 -0.67182324]]\n",
      "###output######## [[0.87044507 0.67182324]]\n",
      "**********************\n",
      "iteration: 22 :::: [[ 0.12921111 -0.66623075]]\n",
      "###output######## [[0.87078889 0.66623075]]\n",
      "**********************\n",
      "iteration: 23 :::: [[ 0.12886751 -0.66058658]]\n",
      "###output######## [[0.87113249 0.66058658]]\n",
      "**********************\n",
      "iteration: 24 :::: [[ 0.12852415 -0.65489414]]\n",
      "###output######## [[0.87147585 0.65489414]]\n",
      "**********************\n",
      "iteration: 25 :::: [[ 0.12818108 -0.64915704]]\n",
      "###output######## [[0.87181892 0.64915704]]\n",
      "**********************\n",
      "iteration: 26 :::: [[ 0.12783833 -0.64337899]]\n",
      "###output######## [[0.87216167 0.64337899]]\n",
      "**********************\n",
      "iteration: 27 :::: [[ 0.12749595 -0.63756387]]\n",
      "###output######## [[0.87250405 0.63756387]]\n",
      "**********************\n",
      "iteration: 28 :::: [[ 0.12715397 -0.63171565]]\n",
      "###output######## [[0.87284603 0.63171565]]\n",
      "**********************\n",
      "iteration: 29 :::: [[ 0.12681245 -0.62583841]]\n",
      "###output######## [[0.87318755 0.62583841]]\n",
      "**********************\n",
      "iteration: 30 :::: [[ 0.12647145 -0.61993633]]\n",
      "###output######## [[0.87352855 0.61993633]]\n",
      "**********************\n",
      "iteration: 31 :::: [[ 0.126131   -0.61401365]]\n",
      "###output######## [[0.873869   0.61401365]]\n",
      "**********************\n",
      "iteration: 32 :::: [[ 0.12579118 -0.60807464]]\n",
      "###output######## [[0.87420882 0.60807464]]\n",
      "**********************\n",
      "iteration: 33 :::: [[ 0.12545203 -0.60212364]]\n",
      "###output######## [[0.87454797 0.60212364]]\n",
      "**********************\n",
      "iteration: 34 :::: [[ 0.12511362 -0.59616497]]\n",
      "###output######## [[0.87488638 0.59616497]]\n",
      "**********************\n",
      "iteration: 35 :::: [[ 0.124776   -0.59020298]]\n",
      "###output######## [[0.875224   0.59020298]]\n",
      "**********************\n",
      "iteration: 36 :::: [[ 0.12443925 -0.58424197]]\n",
      "###output######## [[0.87556075 0.58424197]]\n",
      "**********************\n",
      "iteration: 37 :::: [[ 0.12410341 -0.57828623]]\n",
      "###output######## [[0.87589659 0.57828623]]\n",
      "**********************\n",
      "iteration: 38 :::: [[ 0.12376857 -0.57233995]]\n",
      "###output######## [[0.87623143 0.57233995]]\n",
      "**********************\n",
      "iteration: 39 :::: [[ 0.12343477 -0.5664073 ]]\n",
      "###output######## [[0.87656523 0.5664073 ]]\n",
      "**********************\n",
      "iteration: 40 :::: [[ 0.12310208 -0.56049234]]\n",
      "###output######## [[0.87689792 0.56049234]]\n",
      "**********************\n",
      "iteration: 41 :::: [[ 0.12277056 -0.554599  ]]\n",
      "###output######## [[0.87722944 0.554599  ]]\n",
      "**********************\n",
      "iteration: 42 :::: [[ 0.12244027 -0.54873115]]\n",
      "###output######## [[0.87755973 0.54873115]]\n",
      "**********************\n",
      "iteration: 43 :::: [[ 0.12211128 -0.54289248]]\n",
      "###output######## [[0.87788872 0.54289248]]\n",
      "**********************\n",
      "iteration: 44 :::: [[ 0.12178364 -0.53708658]]\n",
      "###output######## [[0.87821636 0.53708658]]\n",
      "**********************\n",
      "iteration: 45 :::: [[ 0.12145741 -0.53131686]]\n",
      "###output######## [[0.87854259 0.53131686]]\n",
      "**********************\n",
      "iteration: 46 :::: [[ 0.12113264 -0.52558658]]\n",
      "###output######## [[0.87886736 0.52558658]]\n",
      "**********************\n",
      "iteration: 47 :::: [[ 0.12080939 -0.51989886]]\n",
      "###output######## [[0.87919061 0.51989886]]\n",
      "**********************\n",
      "iteration: 48 :::: [[ 0.12048771 -0.51425661]]\n",
      "###output######## [[0.87951229 0.51425661]]\n",
      "**********************\n",
      "iteration: 49 :::: [[ 0.12016765 -0.5086626 ]]\n",
      "###output######## [[0.87983235 0.5086626 ]]\n",
      "**********************\n",
      "iteration: 5951 :::: [[ 0.02111774 -0.02177312]]\n",
      "###output######## [[0.97888226 0.02177312]]\n",
      "**********************\n",
      "iteration: 5952 :::: [[ 0.02111596 -0.02177117]]\n",
      "###output######## [[0.97888404 0.02177117]]\n",
      "**********************\n",
      "iteration: 5953 :::: [[ 0.02111418 -0.02176923]]\n",
      "###output######## [[0.97888582 0.02176923]]\n",
      "**********************\n",
      "iteration: 5954 :::: [[ 0.02111241 -0.02176728]]\n",
      "###output######## [[0.97888759 0.02176728]]\n",
      "**********************\n",
      "iteration: 5955 :::: [[ 0.02111063 -0.02176534]]\n",
      "###output######## [[0.97888937 0.02176534]]\n",
      "**********************\n",
      "iteration: 5956 :::: [[ 0.02110885 -0.0217634 ]]\n",
      "###output######## [[0.97889115 0.0217634 ]]\n",
      "**********************\n",
      "iteration: 5957 :::: [[ 0.02110708 -0.02176146]]\n",
      "###output######## [[0.97889292 0.02176146]]\n",
      "**********************\n",
      "iteration: 5958 :::: [[ 0.0211053  -0.02175952]]\n",
      "###output######## [[0.9788947  0.02175952]]\n",
      "**********************\n",
      "iteration: 5959 :::: [[ 0.02110353 -0.02175758]]\n",
      "###output######## [[0.97889647 0.02175758]]\n",
      "**********************\n",
      "iteration: 5960 :::: [[ 0.02110175 -0.02175564]]\n",
      "###output######## [[0.97889825 0.02175564]]\n",
      "**********************\n",
      "iteration: 5961 :::: [[ 0.02109998 -0.0217537 ]]\n",
      "###output######## [[0.97890002 0.0217537 ]]\n",
      "**********************\n",
      "iteration: 5962 :::: [[ 0.02109821 -0.02175176]]\n",
      "###output######## [[0.97890179 0.02175176]]\n",
      "**********************\n",
      "iteration: 5963 :::: [[ 0.02109643 -0.02174982]]\n",
      "###output######## [[0.97890357 0.02174982]]\n",
      "**********************\n",
      "iteration: 5964 :::: [[ 0.02109466 -0.02174788]]\n",
      "###output######## [[0.97890534 0.02174788]]\n",
      "**********************\n",
      "iteration: 5965 :::: [[ 0.02109289 -0.02174595]]\n",
      "###output######## [[0.97890711 0.02174595]]\n",
      "**********************\n",
      "iteration: 5966 :::: [[ 0.02109112 -0.02174401]]\n",
      "###output######## [[0.97890888 0.02174401]]\n",
      "**********************\n",
      "iteration: 5967 :::: [[ 0.02108934 -0.02174207]]\n",
      "###output######## [[0.97891066 0.02174207]]\n",
      "**********************\n",
      "iteration: 5968 :::: [[ 0.02108757 -0.02174014]]\n",
      "###output######## [[0.97891243 0.02174014]]\n",
      "**********************\n",
      "iteration: 5969 :::: [[ 0.0210858 -0.0217382]]\n",
      "###output######## [[0.9789142 0.0217382]]\n",
      "**********************\n",
      "iteration: 5970 :::: [[ 0.02108403 -0.02173627]]\n",
      "###output######## [[0.97891597 0.02173627]]\n",
      "**********************\n",
      "iteration: 5971 :::: [[ 0.02108226 -0.02173433]]\n",
      "###output######## [[0.97891774 0.02173433]]\n",
      "**********************\n",
      "iteration: 5972 :::: [[ 0.02108049 -0.0217324 ]]\n",
      "###output######## [[0.97891951 0.0217324 ]]\n",
      "**********************\n",
      "iteration: 5973 :::: [[ 0.02107872 -0.02173046]]\n",
      "###output######## [[0.97892128 0.02173046]]\n",
      "**********************\n",
      "iteration: 5974 :::: [[ 0.02107696 -0.02172853]]\n",
      "###output######## [[0.97892304 0.02172853]]\n",
      "**********************\n",
      "iteration: 5975 :::: [[ 0.02107519 -0.0217266 ]]\n",
      "###output######## [[0.97892481 0.0217266 ]]\n",
      "**********************\n",
      "iteration: 5976 :::: [[ 0.02107342 -0.02172467]]\n",
      "###output######## [[0.97892658 0.02172467]]\n",
      "**********************\n",
      "iteration: 5977 :::: [[ 0.02107165 -0.02172274]]\n",
      "###output######## [[0.97892835 0.02172274]]\n",
      "**********************\n",
      "iteration: 5978 :::: [[ 0.02106989 -0.02172081]]\n",
      "###output######## [[0.97893011 0.02172081]]\n",
      "**********************\n",
      "iteration: 5979 :::: [[ 0.02106812 -0.02171887]]\n",
      "###output######## [[0.97893188 0.02171887]]\n",
      "**********************\n",
      "iteration: 5980 :::: [[ 0.02106636 -0.02171695]]\n",
      "###output######## [[0.97893364 0.02171695]]\n",
      "**********************\n",
      "iteration: 5981 :::: [[ 0.02106459 -0.02171502]]\n",
      "###output######## [[0.97893541 0.02171502]]\n",
      "**********************\n",
      "iteration: 5982 :::: [[ 0.02106283 -0.02171309]]\n",
      "###output######## [[0.97893717 0.02171309]]\n",
      "**********************\n",
      "iteration: 5983 :::: [[ 0.02106106 -0.02171116]]\n",
      "###output######## [[0.97893894 0.02171116]]\n",
      "**********************\n",
      "iteration: 5984 :::: [[ 0.0210593  -0.02170923]]\n",
      "###output######## [[0.9789407  0.02170923]]\n",
      "**********************\n",
      "iteration: 5985 :::: [[ 0.02105753 -0.0217073 ]]\n",
      "###output######## [[0.97894247 0.0217073 ]]\n",
      "**********************\n",
      "iteration: 5986 :::: [[ 0.02105577 -0.02170538]]\n",
      "###output######## [[0.97894423 0.02170538]]\n",
      "**********************\n",
      "iteration: 5987 :::: [[ 0.02105401 -0.02170345]]\n",
      "###output######## [[0.97894599 0.02170345]]\n",
      "**********************\n",
      "iteration: 5988 :::: [[ 0.02105225 -0.02170152]]\n",
      "###output######## [[0.97894775 0.02170152]]\n",
      "**********************\n",
      "iteration: 5989 :::: [[ 0.02105048 -0.0216996 ]]\n",
      "###output######## [[0.97894952 0.0216996 ]]\n",
      "**********************\n",
      "iteration: 5990 :::: [[ 0.02104872 -0.02169767]]\n",
      "###output######## [[0.97895128 0.02169767]]\n",
      "**********************\n",
      "iteration: 5991 :::: [[ 0.02104696 -0.02169575]]\n",
      "###output######## [[0.97895304 0.02169575]]\n",
      "**********************\n",
      "iteration: 5992 :::: [[ 0.0210452  -0.02169383]]\n",
      "###output######## [[0.9789548  0.02169383]]\n",
      "**********************\n",
      "iteration: 5993 :::: [[ 0.02104344 -0.0216919 ]]\n",
      "###output######## [[0.97895656 0.0216919 ]]\n",
      "**********************\n",
      "iteration: 5994 :::: [[ 0.02104168 -0.02168998]]\n",
      "###output######## [[0.97895832 0.02168998]]\n",
      "**********************\n",
      "iteration: 5995 :::: [[ 0.02103992 -0.02168806]]\n",
      "###output######## [[0.97896008 0.02168806]]\n",
      "**********************\n",
      "iteration: 5996 :::: [[ 0.02103816 -0.02168614]]\n",
      "###output######## [[0.97896184 0.02168614]]\n",
      "**********************\n",
      "iteration: 5997 :::: [[ 0.0210364  -0.02168422]]\n",
      "###output######## [[0.9789636  0.02168422]]\n",
      "**********************\n",
      "iteration: 5998 :::: [[ 0.02103465 -0.0216823 ]]\n",
      "###output######## [[0.97896535 0.0216823 ]]\n",
      "**********************\n",
      "iteration: 5999 :::: [[ 0.02103289 -0.02168037]]\n",
      "###output######## [[0.97896711 0.02168037]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(iteration):\n",
    "\n",
    "    hidden_layer=np.dot(input,hidden_weights) \n",
    "    hidden_layer=sigmoid(hidden_layer+hidden_biass)\n",
    "\n",
    "    output_layer=np.dot(hidden_layer,output_weights) \n",
    "    output_layer=sigmoid(output_layer+output_bias)\n",
    "\n",
    "    error = (output-output_layer) \n",
    "    gradient_outputLayer=gradient(output_layer)\n",
    "    error_terms_output=gradient_outputLayer * error \n",
    "    error_terms_hidden=gradient(hidden_layer)*np.dot(error_terms_output,output_weights.T)\n",
    "\n",
    "    gradient_hidden_weights = np.dot(input.reshape(inputNeurons,1),error_terms_hidden.reshape(1,hiddenlayerNeurons))\n",
    "    gradient_ouput_weights = np.dot(hidden_layer.reshape(hiddenlayerNeurons,1),error_terms_output.reshape(1,outputNeurons))\n",
    "\n",
    "    hidden_weights = hidden_weights + 0.05*gradient_hidden_weights \n",
    "    output_weights = output_weights + 0.05*gradient_ouput_weights \n",
    "    if i<50 or i>iteration-50:\n",
    "        print(\"**********************\") \n",
    "        print(\"iteration:\",i,\"::::\",error) \n",
    "        print(\"###output########\",output_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
